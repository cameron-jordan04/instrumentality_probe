{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9669118c",
   "metadata": {},
   "source": [
    "### Alternative Steering Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037cc951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy import stats\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_INPUT_FILE = \"results/base_activation_dataset.parquet\"\n",
    "INSTRUCT_INPUT_FILE = \"results/instruct_activation_dataset.parquet\"\n",
    "\n",
    "BASE_VECTORS_DIR = \"steering_vectors/{method}/base\"\n",
    "INSTRUCT_VECTORS_DIR = \"steering_vectors/{method}/instruct\"\n",
    "\n",
    "METHODS_TO_ANALYZE = ['caa', 'repe', 'lda', 'svm', 'lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45a531b",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbd723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "\n",
    "# Unfortunately, shared prompt prefixes are not saved in the activation dataset, so in order to compute mean of differences and representation engineering steering vectors (which rely on pairwise matching) we need to recover the pairs using \n",
    "def get_smart_pair_map(df, start_len=50, step=10):\n",
    "    \"\"\"\n",
    "    Returns a dictionary mapping {prompt_string: pair_id} using a recursive\n",
    "    zoom-in strategy to disentangle prompts with shared prefixes.\n",
    "    \"\"\"\n",
    "    # 1. Get unique rows (prompt + label) to avoid processing duplicate layers\n",
    "    # We only care about the unique text-label combinations\n",
    "    unique_df = df[['prompt', 'label']].drop_duplicates().copy()\n",
    "    \n",
    "    pair_map = {}\n",
    "    current_pair_id = 0\n",
    "    \n",
    "    def recursive_match(subset_df, prefix_len):\n",
    "        nonlocal current_pair_id\n",
    "        \n",
    "        # Stop if subset is empty\n",
    "        if subset_df.empty:\n",
    "            return\n",
    "\n",
    "        # Stop if we've exceeded the length of the shortest prompt\n",
    "        # (This handles the edge case where prompts are literally identical)\n",
    "        min_prompt_len = subset_df['prompt'].str.len().min()\n",
    "        if prefix_len > min_prompt_len:\n",
    "             print(f\"    Warning: Ambiguous prompts found at max length {min_prompt_len}. Skipping.\")\n",
    "             return\n",
    "\n",
    "        # Group by the current prefix\n",
    "        subset_df['temp_prefix'] = subset_df['prompt'].str.slice(0, prefix_len)\n",
    "        groups = subset_df.groupby('temp_prefix')\n",
    "        \n",
    "        for prefix, group in groups:\n",
    "            # Count distribution\n",
    "            counts = group['label'].value_counts()\n",
    "            n_inst = counts.get('instrumental', 0)\n",
    "            n_term = counts.get('terminal', 0)\n",
    "            \n",
    "            # CASE A: Perfect Pair (1 vs 1) -> ASSIGN ID\n",
    "            if n_inst == 1 and n_term == 1:\n",
    "                for prompt in group['prompt']:\n",
    "                    pair_map[prompt] = current_pair_id\n",
    "                current_pair_id += 1\n",
    "                \n",
    "            # CASE B: Too many items (e.g. 2 vs 2) -> ZOOM IN (Recurse)\n",
    "            elif n_inst > 1 and n_term > 1:\n",
    "                # We assume the prefix is longer, so we try +step chars\n",
    "                # Pass ONLY this group to the next level\n",
    "                recursive_match(group.copy(), prefix_len + step)\n",
    "                \n",
    "            # CASE C: Unbalanced (e.g. 2 vs 1, or 1 vs 0) -> DISCARD (Orphan)\n",
    "            else:\n",
    "                # If we are deep in recursion, this might be a fail state.\n",
    "                # But sometimes unbalanced groups split into balanced ones deeper down?\n",
    "                # No, if we have 2 Inst and 1 Term, we can never form 2 pairs.\n",
    "                # However, we might form 1 pair and drop 1 orphan.\n",
    "                if n_inst >= 1 and n_term >= 1:\n",
    "                    recursive_match(group.copy(), prefix_len + step)\n",
    "                # Else: completely broken (e.g. 3 inst, 0 term), ignore.\n",
    "                pass\n",
    "\n",
    "    print(f\"Starting recursive pairing (min_len={start_len}, step={step})...\")\n",
    "    recursive_match(unique_df, start_len)\n",
    "    print(f\"Matched {len(pair_map) // 2} pairs from {len(unique_df)} unique prompts.\")\n",
    "    \n",
    "    return pair_map\n",
    "\n",
    "def recover_pairings(df):\n",
    "    \"\"\"\n",
    "    Applies the smart pairing map to the dataframe.\n",
    "    \"\"\"\n",
    "    # Generate the map based on unique prompts\n",
    "    pair_map = get_smart_pair_map(df)\n",
    "    \n",
    "    # Map IDs\n",
    "    df['pair_id'] = df['prompt'].map(pair_map)\n",
    "    \n",
    "    # Drop rows that weren't paired\n",
    "    before = len(df)\n",
    "    df_clean = df.dropna(subset=['pair_id']).copy()\n",
    "    df_clean['pair_id'] = df_clean['pair_id'].astype(int)\n",
    "    \n",
    "    removed = before - len(df_clean)\n",
    "    if removed > 0:\n",
    "        print(f\"  Dropped {removed} activations (orphans or duplicates).\")\n",
    "        \n",
    "    return df_clean\n",
    "\n",
    "def get_aligned_data(df, layer_name, activation_cols):\n",
    "    \"\"\"\n",
    "    Extracts aligned instrumental and terminal matrices using 'pair_id'.\n",
    "    \"\"\"\n",
    "    layer_df = df[df['layer'] == layer_name].copy()\n",
    "    \n",
    "    # Sort by pair_id to ensure row alignment\n",
    "    # We allow instrumental/terminal to be in any order within the pair,\n",
    "    # so we sort by pair_id first, then by label to ensure consistency.\n",
    "    # e.g. (Pair 0, Instrumental), (Pair 0, Terminal), (Pair 1, Inst)...\n",
    "    layer_df = layer_df.sort_values(by=['pair_id', 'label'])\n",
    "    \n",
    "    inst_df = layer_df[layer_df['label'] == 'instrumental']\n",
    "    term_df = layer_df[layer_df['label'] == 'terminal']\n",
    "    \n",
    "    # Sanity check: Ensure IDs match exactly row-for-row\n",
    "    if not np.array_equal(inst_df['pair_id'].values, term_df['pair_id'].values):\n",
    "        # Fallback: force intersection\n",
    "        common_ids = np.intersect1d(inst_df['pair_id'].values, term_df['pair_id'].values)\n",
    "        inst_df = inst_df[inst_df['pair_id'].isin(common_ids)]\n",
    "        term_df = term_df[term_df['pair_id'].isin(common_ids)]\n",
    "        \n",
    "    X_pos = inst_df[activation_cols].values.astype(np.float32)\n",
    "    X_neg = term_df[activation_cols].values.astype(np.float32)\n",
    "    \n",
    "    return X_pos, X_neg\n",
    "\n",
    "## Other Helper Functions\n",
    "\n",
    "def save_vector(vector, method, model_type, layer_name):\n",
    "    \"\"\"\n",
    "    Saves the vector to the appropriate directory based on config.\n",
    "    \"\"\"\n",
    "    if model_type == 'base':\n",
    "        out_dir = BASE_VECTORS_DIR.format(method=method)\n",
    "    else:\n",
    "        out_dir = INSTRUCT_VECTORS_DIR.format(method=method)\n",
    "        \n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    np.save(os.path.join(out_dir, f\"{layer_name}_steering.npy\"), vector)\n",
    "\n",
    "def cohens_d(group1, group2):\n",
    "    \"\"\"\n",
    "    Calculates Cohen's D (effect size) for independent samples.\n",
    "    Assumes roughly equal variances, which is reasonable here.\n",
    "    \"\"\"\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n",
    "    \n",
    "    # Pooled standard deviation\n",
    "    pooled_se = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "    \n",
    "    # Calculate d\n",
    "    d = (np.mean(group1) - np.mean(group2)) / pooled_se\n",
    "    return d\n",
    "\n",
    "def load_layer_vectors(layer_name, model_type):\n",
    "    \"\"\"\n",
    "    Loads all available steering vectors for a specific layer.\n",
    "    Returns a dictionary of vectors and a list of valid method names found.\n",
    "    \"\"\"\n",
    "    vector_dict = {}\n",
    "    valid_methods = []\n",
    "    \n",
    "    if model_type == 'base':\n",
    "        base_dir = BASE_VECTORS_DIR\n",
    "    else:\n",
    "        base_dir = INSTRUCT_VECTORS_DIR\n",
    "        \n",
    "    for method in METHODS_TO_ANALYZE:\n",
    "        path = os.path.join(base_dir.format(method=method), f\"{layer_name}_steering.npy\")\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                vector_dict[method] = np.load(path)\n",
    "                valid_methods.append(method)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {method} for {layer_name}: {e}\")\n",
    "\n",
    "    return vector_dict, valid_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f17136",
   "metadata": {},
   "source": [
    "#### Load .parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad092b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset Loading ---\n",
    "datasets = {}\n",
    "\n",
    "print(f\"Loading Base model activations from {BASE_INPUT_FILE}\")\n",
    "df_base = pd.read_parquet(BASE_INPUT_FILE)\n",
    "datasets['base'] = recover_pairings(df_base)\n",
    "\n",
    "print(f\"Loading Instruct model activations from {INSTRUCT_INPUT_FILE}\")\n",
    "df_instruct = pd.read_parquet(INSTRUCT_INPUT_FILE)\n",
    "datasets['instruct'] = recover_pairings(df_instruct)\n",
    "\n",
    "activation_cols = [col for col in datasets['base'].columns if col.startswith('act_')]\n",
    "hidden_dim = len(activation_cols)\n",
    "\n",
    "layer_names = datasets['base']['layer'].unique()\n",
    "sorted_layer_names = sorted(layer_names, key=lambda x: int(x.split('.')[-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89597fb5",
   "metadata": {},
   "source": [
    "#### Compute CAA (Difference of Means (DoM)) Steering Vectors\n",
    "Computes $\\mu_{instrumental} - \\mu_{terminal}$ across the entire dataset of activations at a particular layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4501eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "METHOD = \"caa\"\n",
    "print(f\"--- Computing {METHOD.upper()} Vectors ---\")\n",
    "\n",
    "for model_type, df in datasets.items():\n",
    "    print(f\"Processing {model_type} model...\")\n",
    "    \n",
    "    for layer in sorted_layer_names:\n",
    "        # 1. Get Aligned Data\n",
    "        X_pos, X_neg = get_aligned_data(df, layer, activation_cols)\n",
    "        \n",
    "        # 2. Compute CAA: Mean(Pos) - Mean(Neg)\n",
    "        mean_pos = np.mean(X_pos, axis=0)\n",
    "        mean_neg = np.mean(X_neg, axis=0)\n",
    "        vec_caa = mean_pos - mean_neg\n",
    "        \n",
    "        # 3. Save\n",
    "        save_vector(vec_caa, METHOD, model_type, layer)\n",
    "\n",
    "print(\"CAA computation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b0298",
   "metadata": {},
   "source": [
    "#### Compute (Linear) Support Vector Machine (SVM) Steering Vectors\n",
    "Compute the Linear SVM; the direction that maximizes the margin (gap) between the opposing classes, producing a steering vector defined by the boundary of the concept rather than its average, which may help to ignore irrelevant noise from \"easy\" examples that often skew mean-based or probabilistic methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca70c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "METHOD = \"svm\"\n",
    "print(f\"--- Computing {METHOD.upper()} Vectors ---\")\n",
    "\n",
    "for model_type, df in datasets.items():\n",
    "    print(f\"Processing {model_type} model...\")\n",
    "    \n",
    "    for layer in sorted_layer_names:\n",
    "        X_pos, X_neg = get_aligned_data(df, layer, activation_cols)\n",
    "        \n",
    "        # 1. Prepare Training Data\n",
    "        # SVM requires stacked samples (N_samples, N_features) and labels\n",
    "        X_train = np.vstack([X_pos, X_neg])\n",
    "        \n",
    "        # Labels: 1 for Instrumental (Pos), 0 for Terminal (Neg)\n",
    "        y_train = np.concatenate([np.ones(len(X_pos)), np.zeros(len(X_neg))])\n",
    "        \n",
    "        # 2. Fit Linear SVM\n",
    "        # C=0.01: Strong regularization to prevent overfitting in high dimensions\n",
    "        # dual=\"auto\": Lets sklearn choose the best solver for n_samples vs n_features\n",
    "        svm = LinearSVC(C=0.01, fit_intercept=True, dual=\"auto\", max_iter=10000, random_state=42)\n",
    "        svm.fit(X_train, y_train)\n",
    "        \n",
    "        # 3. Extract Normal Vector\n",
    "        vec_svm = svm.coef_[0]\n",
    "        \n",
    "        # 4. Sign/Direction Check\n",
    "        mean_diff_direction = np.mean(X_pos, axis=0) - np.mean(X_neg, axis=0)\n",
    "        if np.dot(vec_svm, mean_diff_direction) < 0:\n",
    "            vec_svm = -vec_svm\n",
    "            \n",
    "        save_vector(vec_svm, METHOD, model_type, layer)\n",
    "\n",
    "print(\"SVM computation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4303a1",
   "metadata": {},
   "source": [
    "#### Compute RepE Steering Vectors\n",
    "Computes PCA of a set of difference vectors $\\{x_{instrumental, i} - x_{terminal, i}\\}_{i=1}^N$, and then uses the first principal component vector as the steering vector (this is the direction of maximum variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c029ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "METHOD = \"repe\"\n",
    "print(f\"--- Computing {METHOD.upper()} Vectors ---\")\n",
    "\n",
    "for model_type, df in datasets.items():\n",
    "    print(f\"Processing {model_type} model...\")\n",
    "    \n",
    "    for layer in sorted_layer_names:\n",
    "        X_pos, X_neg = get_aligned_data(df, layer, activation_cols)\n",
    "        \n",
    "        # 1. PCA on Difference Vectors\n",
    "        pairwise_diffs = X_pos - X_neg\n",
    "        \n",
    "        svd = TruncatedSVD(n_components=1)\n",
    "        svd.fit(pairwise_diffs)\n",
    "        vec_repe = svd.components_[0]\n",
    "        \n",
    "        # 2. Sign Correction\n",
    "        # We check against the simple mean difference to ensure the vector points towards \"Instrumental\"\n",
    "        mean_diff_direction = np.mean(X_pos, axis=0) - np.mean(X_neg, axis=0)\n",
    "        if np.dot(vec_repe, mean_diff_direction) < 0:\n",
    "            vec_repe = -vec_repe\n",
    "            \n",
    "        save_vector(vec_repe, METHOD, model_type, layer)\n",
    "\n",
    "print(\"RepE computation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee636fa7",
   "metadata": {},
   "source": [
    "#### Compute LDA Steering Vectors\n",
    "(also called a \"Mass Mean Probe\") Computes $(\\Sigma + \\lambda I)^{-1}(\\mu_{instrumental} - \\mu_{terminal})$, where $\\Sigma$ is the pooled covariance matrix of the activations, which works to effectively \"whiten\" the noise in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3f45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "METHOD = \"lda\"\n",
    "print(f\"--- Computing {METHOD.upper()} Vectors ---\")\n",
    "REG_PARAM = 1e-4  # Regularization to prevent singular matrices\n",
    "\n",
    "for model_type, df in datasets.items():\n",
    "    print(f\"Processing {model_type} model...\")\n",
    "    \n",
    "    for layer in sorted_layer_names:\n",
    "        X_pos, X_neg = get_aligned_data(df, layer, activation_cols)\n",
    "        \n",
    "        # 1. Compute Pooled Covariance\n",
    "        # Adding shrinkage (eye * REG_PARAM) for numerical stability\n",
    "        cov_pos = np.cov(X_pos, rowvar=False) + (np.eye(hidden_dim) * REG_PARAM)\n",
    "        cov_neg = np.cov(X_neg, rowvar=False) + (np.eye(hidden_dim) * REG_PARAM)\n",
    "        pooled_cov = (cov_pos + cov_neg) / 2\n",
    "        \n",
    "        # 2. Compute Difference of Means\n",
    "        diff_means = np.mean(X_pos, axis=0) - np.mean(X_neg, axis=0)\n",
    "        \n",
    "        # 3. Solve Σx = μ_diff (equivalent to x = Σ^-1 * μ_diff)\n",
    "        try:\n",
    "            vec_lda = np.linalg.solve(pooled_cov, diff_means)\n",
    "            # Normalize to unit length\n",
    "            vec_lda = vec_lda / np.linalg.norm(vec_lda)\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(f\"  LDA failed for {layer}, defaulting to CAA direction.\")\n",
    "            vec_lda = diff_means\n",
    "            \n",
    "        save_vector(vec_lda, METHOD, model_type, layer)\n",
    "\n",
    "print(\"LDA computation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1130a226",
   "metadata": {},
   "source": [
    "#### Evaluate Steering Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7c6388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity_heatmap(layer_name, model_type='base'):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of cosine similarities between different steering vectors.\n",
    "    \"\"\"\n",
    "    vectors, methods = load_layer_vectors(layer_name, model_type)\n",
    "    \n",
    "    if len(methods) < 2:\n",
    "        print(f\"Not enough vectors found for layer {layer_name} to plot heatmap.\")\n",
    "        return\n",
    "\n",
    "    # Stack vectors into a matrix (N_methods, Hidden_Dim)\n",
    "    vec_matrix = np.array([vectors[m] for m in methods])\n",
    "\n",
    "    # Compute similarity matrix\n",
    "    # Result is (N_methods, N_methods)\n",
    "    sim_matrix = cosine_similarity(vec_matrix)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Using vmin=-1, vmax=1 to handle anti-correlated vectors\n",
    "    sns.heatmap(sim_matrix, annot=True, xticklabels=methods, yticklabels=methods, \n",
    "                cmap=\"RdBu_r\", vmin=-1, vmax=1, fmt=\".3f\", center=0)\n",
    "    \n",
    "    plt.title(f\"Steering Vector Alignment (Cosine Sim)\\nLayer: {layer_name} | Model: {model_type.capitalize()}\")\n",
    "    plt.tight_layout()\n",
    "    fig_name = f\"figures/{model_type}_steering_vector_similarity.png\"\n",
    "    plt.savefig(fig_name, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# ===========================\n",
    "# RUN ANALYSIS 1\n",
    "# ===========================\n",
    "TARGET_LAYER = 'model.layers.13.mlp'\n",
    "\n",
    "for model_type in ['base', 'instruct']:\n",
    "    plot_similarity_heatmap(TARGET_LAYER, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f550e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_separation_analysis(layer_name, model_type, datasets_dict):\n",
    "    \"\"\"\n",
    "    Plots histograms of projected activations and calculates separation statistics.\n",
    "    \"\"\"\n",
    "    # 1. Get Data and Vectors\n",
    "    df = datasets_dict[model_type]\n",
    "    X_pos, X_neg = get_aligned_data(df, layer_name, activation_cols)\n",
    "    vectors, methods = load_layer_vectors(layer_name, model_type)\n",
    "    \n",
    "    if not methods:\n",
    "        print(f\"No vectors found for {layer_name}.\")\n",
    "        return\n",
    "\n",
    "    # 2. Setup Plot Grid (2 rows, 3 columns)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes_flat = axes.flatten()\n",
    "    \n",
    "    stats_data = []\n",
    "\n",
    "    # 3. Iterate through methods and plot\n",
    "    for i, ax in enumerate(axes_flat[:5]): # Only use first 5 slots\n",
    "        if i < len(methods):\n",
    "            method = methods[i]\n",
    "            vec = vectors[method]\n",
    "            \n",
    "            # --- Project Data ---\n",
    "            # Dot product: (N_samples, Hidden) @ (Hidden,) -> (N_samples,)\n",
    "            # This results in a single scalar \"steering score\" for each input\n",
    "            proj_pos = X_pos @ vec\n",
    "            proj_neg = X_neg @ vec\n",
    "            \n",
    "            # Ensure positive mean is higher for consistency in plots/stats\n",
    "            if np.mean(proj_pos) < np.mean(proj_neg):\n",
    "                proj_pos = -proj_pos\n",
    "                proj_neg = -proj_neg\n",
    "\n",
    "            # --- Plot Histogram ---\n",
    "            # We calculate bins based on combined data range for fair comparison\n",
    "            combined_data = np.concatenate([proj_pos, proj_neg])\n",
    "            bins = np.linspace(np.min(combined_data), np.max(combined_data), 40)\n",
    "            \n",
    "            ax.hist(proj_pos, bins=bins, alpha=0.6, label='Instrumental', color='red', density=True)\n",
    "            ax.hist(proj_neg, bins=bins, alpha=0.6, label='Terminal', color='blue', density=True)\n",
    "            \n",
    "            ax.set_title(f\"{method.upper()} Projection\", fontsize=12, fontweight='bold')\n",
    "            ax.set_yticks([]) # Remove y-axis ticks for cleaner look\n",
    "            if i == 0: ax.legend()\n",
    "            \n",
    "            # --- Calculate Stats ---\n",
    "            # Cohen's D (Effect Size)\n",
    "            d_score = cohens_d(proj_pos, proj_neg)\n",
    "            # T-test p-value (measuring if means are significantly different)\n",
    "            t_stat, p_val = stats.ttest_ind(proj_pos, proj_neg, equal_var=True)\n",
    "            \n",
    "            stats_data.append({\n",
    "                'Method': method.upper(),\n",
    "                \"Cohen's D\": d_score,\n",
    "                \"P-Value\": p_val\n",
    "            })\n",
    "        else:\n",
    "            # Hide unused axes if fewer than 5 methods found\n",
    "            ax.axis('off')\n",
    "\n",
    "    # 4. Fill the 6th slot with Statistics Table\n",
    "    ax_stats = axes_flat[5]\n",
    "    ax_stats.axis('off')\n",
    "    ax_stats.set_title(\"Separation Statistics\", fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Format text for table\n",
    "    header = f\"{'Method':<10} | {'Cohen\\'s D':<10} | {'P-Value (log10)':<15}\"\n",
    "    separator = \"-\"*45\n",
    "    row_txt = [header, separator]\n",
    "    \n",
    "    for item in stats_data:\n",
    "        # Use log10 for p-value as it will likely be extremely small (e.g. 1e-100)\n",
    "        pval_log = np.log10(item['P-Value'] + 1e-300) # avoid log(0)\n",
    "        row = f\"{item['Method']:<10} | {item['Cohen\\'s D']:<10.4f} | {pval_log:<15.2f}\"\n",
    "        row_txt.append(row)\n",
    "        \n",
    "    full_txt = \"\\n\".join(row_txt)\n",
    "    # Place text in the middle of the empty subplot box\n",
    "    ax_stats.text(0.1, 0.5, full_txt, fontsize=11, family='monospace', va='center')\n",
    "\n",
    "    plt.suptitle(f\"Separation Power Analysis\\nLayer {layer_name} | Model: {model_type.capitalize()}\", fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    fig_name = f\"figures/{model_type}_separation_histogram.png\"\n",
    "    plt.savefig(fig_name, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# RUN ANALYSIS 2\n",
    "# ===========================\n",
    "\n",
    "# Assumes 'datasets' dict from previous steps exists in notebook memory\n",
    "TARGET_LAYER = 'model.layers.13.mlp' \n",
    "\n",
    "for model_type in ['base', 'instruct']:\n",
    "    plot_separation_analysis(TARGET_LAYER, model_type, datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
